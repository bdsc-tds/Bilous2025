{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.11/site-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 4, in <module>\n",
      "  File \"/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py\", line 295, in __getattr__\n",
      "    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n",
      "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n",
      "\n",
      "CUDA driver library cannot be found.\n",
      "If you are sure that a CUDA driver is installed,\n",
      "try setting environment variable NUMBA_CUDA_DRIVER\n",
      "with the file path of the CUDA driver shared library.\n",
      ":\n",
      "\n",
      "\n",
      "Not patching Numba\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.11/site-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
      "  warnings.warn(str(e))\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.11/site-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
      "  warnings.warn(str(e))\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.11/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import spatialdata_io\n",
    "import spatialdata\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sys\n",
    "sys.path.extend(['../../scripts','../../scripts/xenium'])\n",
    "from QC import QC\n",
    "import coexpression\n",
    "import readwrite\n",
    "cfg = readwrite.config()\n",
    "\n",
    "def read_cc_file(k, method, target_count, results_dir):\n",
    "    \"\"\"\n",
    "    Worker function to read Parquet files for a specific method and target count.\n",
    "    \"\"\"\n",
    "    out_file_coexpr = results_dir / f'coexpression/{\"/\".join(k)}/coexpression_{method}_{target_count}.parquet'\n",
    "    out_file_pos_rate = results_dir / f'coexpression/{\"/\".join(k)}/positivity_rate_{method}_{target_count}.parquet'\n",
    "    \n",
    "    cc = pd.read_parquet(out_file_coexpr)\n",
    "    pos_rate = pd.read_parquet(out_file_pos_rate)[0]\n",
    "    return method, target_count, cc, pos_rate\n",
    "\n",
    "\n",
    "def read_cc_files(cc_paths, results_dir):\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(read_cc_file, k, method, target_count, results_dir)\n",
    "            for k, method, target_count in cc_paths\n",
    "        ]\n",
    "\n",
    "        CC = {}\n",
    "        pos_rate = {}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            method, target_count, cc, pr = future.result()\n",
    "            k = cc_paths[futures.index(future)][0]  # Retrieve the `k` corresponding to this future\n",
    "            \n",
    "            if k not in CC:\n",
    "                CC[k] = {}\n",
    "            if k not in pos_rate:\n",
    "                pos_rate[k] = {}\n",
    "\n",
    "            CC[k][method, target_count] = cc\n",
    "            pos_rate[k][method, target_count] = pr\n",
    "    return CC, pos_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTS_PATH = \"/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/xenium_paper/data/xenium/processed/segmentation/10x_0um/breast/breast/0OE1/0OE1/normalised_results/outs/cell_feature_matrix.h5\"\n",
    "# TRANSCRIPT_PATH = \"/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/xenium_paper/data/xenium/processed/segmentation/10x_0um/breast/breast/0OE1/0OE1/normalised_results/outs/transcripts.parquet\"\n",
    "# RCTD_PATH = \"/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/xenium_paper/data/xenium/processed/segmentation/10x_0um/breast/breast/0OE1/0OE1/cell_type_annotation/reference_based/matched_reference/rctd/Level1/single_cell/labels.csv\"\n",
    "\n",
    "# ad = sc.read_10x_h5(COUNTS_PATH)\n",
    "# transcripts = pd.read_parquet(TRANSCRIPT_PATH)\n",
    "# obs = pd.read_csv(RCTD_PATH,index_col=0)\n",
    "\n",
    "\n",
    "# print(len(ad),len(transcripts['cell_id'].unique())-1)\n",
    "# X2 = transcripts.groupby(['feature_name', 'cell_id']).size().unstack(fill_value=0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg paths\n",
    "xenium_dir = Path(cfg['xenium_processed_data_dir'])\n",
    "xenium_raw_data_dir = Path(cfg['xenium_raw_data_dir'])\n",
    "results_dir = Path(cfg['results_dir'])\n",
    "\n",
    "# Segmentation, mapping paths\n",
    "dir_segmentations = {\n",
    "    dir_segmentation.name: (dir_segmentation)\n",
    "    for dir_segmentation in xenium_dir.iterdir()\n",
    "}\n",
    "# dir_segmentations['default'] = xenium_raw_data_dir\n",
    "\n",
    "# Params\n",
    "# SEGMENTATIONS = list(dir_segmentations.keys())\n",
    "# COHORTS = [p.stem for p in xenium_raw_data_dir.iterdir() if p.is_dir()]\n",
    "# COHORTS_PANELS = {cohort: [p.stem for p in (xenium_raw_data_dir / cohort).iterdir()] for cohort in COHORTS}\n",
    "# COHORTS_SAMPLES = {(cohort,panel): \n",
    "#                         [replicate.stem \n",
    "#                         for sample in (xenium_raw_data_dir / cohort / panel).iterdir()\n",
    "#                         for replicate in sample.iterdir()\n",
    "#                         if 'corrupt' not in replicate.name and 'output' not in replicate.name and replicate.is_dir()] \n",
    "#                     for cohort in COHORTS \n",
    "#                     for panel in COHORTS_PANELS[cohort]}\n",
    "\n",
    "\n",
    "# Read resegmentations and RCTD\n",
    "# Coexpression params\n",
    "methods = ['conditional','jaccard','pearson','spearman']\n",
    "target_counts = [30,50]\n",
    "\n",
    "xenium_paths = {}\n",
    "umaps = {}\n",
    "cc_paths = []\n",
    "\n",
    "for segmentation in (segmentations := xenium_dir.iterdir()):\n",
    "    for cohort in (cohorts := segmentation.iterdir()): \n",
    "        for panel in (panels := cohort.iterdir()):\n",
    "            for sample in (samples := panel.iterdir()):\n",
    "                for replicate in (replicates := sample.iterdir()):\n",
    "                    \n",
    "                    k = (segmentation.stem,cohort.stem,panel.stem,sample.stem,replicate.stem)\n",
    "                    replicate_path = replicate / \"normalised_results/outs\"\n",
    "                    name = '/'.join(k)\n",
    "\n",
    "                    xenium_paths[k] = replicate_path\n",
    "                    if (replicate_path / 'analysis/umap/gene_expression_2_components/projection.csv').exists():\n",
    "                        umaps[k] = pd.read_csv(replicate_path / 'analysis/umap/gene_expression_2_components/projection.csv',index_col=0)\n",
    "\n",
    "                    if replicate_path.exists():\n",
    "\n",
    "                        for method in methods:\n",
    "                            for target_count in target_counts:\n",
    "                                cc_paths.append((k, method, target_count))\n",
    "\n",
    "\n",
    "# CC, pos_rate = read_cc_files(cc_paths, results_dir)\n",
    "xenium_levels = ('segmentation','cohort','panel','sample','replicate')\n",
    "# ads = readwrite.read_xenium_samples(xenium_paths,anndata_only=True,transcripts=False,sample_name_as_key=False)\n",
    "# ads = pd.Series(ads.values(),\n",
    "#                 index=pd.Index(ads.keys(),name = xenium_levels),\n",
    "#                 dtype=object).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name,f in all_files.items():\n",
    "#     if pathlib.Path(cfg['figures_dir']+f'xenium/QC/{name}.png').exists():\n",
    "#         print('skipping',name,f)\n",
    "#         continue\n",
    "#     else:\n",
    "#         # try:\n",
    "#         # sd = readwrite.read_xenium_samples({name:f},anndata_only=False,transcripts=True)[name]\n",
    "#         QC(sd,name,cfg['figures_dir']+f'xenium/QC/{name}.png')\n",
    "#         %xdel sd\n",
    "#         # except:\n",
    "#         #     print('failed',name,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_samples = ['1G4Y', '1G11']\n",
    "# for name,f in all_files.items():\n",
    "#     if name.split('-')[2] in failed_samples:\n",
    "#         print(name)\n",
    "#         sd = readwrite.read_xenium({name:f},anndata_only=False,transcripts=True)[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coexpression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find spurious genes as in proseg paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coexpression params\n",
    "min_positivity_rate = 0.01\n",
    "cc_cutoff = 1.5\n",
    "log2 = False\n",
    "ref_segmentation = '10x_0um'\n",
    "\n",
    "CCdiff = {}\n",
    "spurious_gene_pairs = {}\n",
    "for k in CC.keys():\n",
    "    if k[0] == ref_segmentation:\n",
    "        continue\n",
    "\n",
    "    CCdiff[k] = {}\n",
    "    spurious_gene_pairs[k] = {}\n",
    "    for m in methods:\n",
    "        for t in target_counts:\n",
    "            k_ref = (ref_segmentation,*k[1:])\n",
    "\n",
    "            (CCdiff[k][m,t], \n",
    "            spurious_gene_pairs[k][m,t]) = coexpression.compare_segmentations(\n",
    "                                CC_ref_seg = CC[k_ref][m,t], \n",
    "                                CC_other_seg = CC[k][m,t], \n",
    "                                pos_rate_ref_seg=pos_rate[k_ref][m,t], \n",
    "                                pos_rate_other_seg=pos_rate[k][m,t], \n",
    "                                min_positivity_rate=min_positivity_rate, \n",
    "                                cc_cutoff = cc_cutoff,\n",
    "                                method=m, log2=log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "method = 'jaccard'\n",
    "target_count = 50\n",
    "ref_oversegmentation = '10x_15um'\n",
    "log2 = True\n",
    "keys = pd.DataFrame(CCdiff.keys(),columns=xenium_levels)\n",
    "cohort_panels = keys[['cohort','panel']].drop_duplicates()\n",
    "\n",
    "for method in methods:\n",
    "    for target_count in target_counts:\n",
    "        for cohort,panel in cohort_panels.values:\n",
    "            data = []\n",
    "            for _,k in keys.iterrows():\n",
    "                if (cohort == k['cohort'] and panel ==  k['panel']):\n",
    "                    \n",
    "                    k_ref_over = (ref_oversegmentation,*k[1:])\n",
    "\n",
    "                    mat = CCdiff[*k][method,target_count].replace(1.,np.nan)\n",
    "                    if log2:\n",
    "                        with warnings.catch_warnings(action=\"ignore\"):\n",
    "                            mat = np.log2(mat)\n",
    "                        \n",
    "                    i = spurious_gene_pairs[k_ref_over][method,target_count][:,0]\n",
    "                    j = spurious_gene_pairs[k_ref_over][method,target_count][:,1]\n",
    "                    flat_values = mat.values[mat.index.get_indexer(i), mat.columns.get_indexer(j)] \n",
    "                    data.extend(np.hstack((np.tile(k,(len(i),1)),flat_values[:,None])))\n",
    "\n",
    "            # Convert to DataFrame for easier plotting\n",
    "            df = pd.DataFrame(data, columns=xenium_levels+('log2 relative coexpression',))\n",
    "            df['log2 relative coexpression']=df['log2 relative coexpression'].astype(float)\n",
    "\n",
    "            # Create joint boxplot\n",
    "            sns.set(style=\"ticks\",palette='colorblind')\n",
    "            plt.figure(figsize=(6, df['sample'].nunique()))\n",
    "            g = sns.violinplot(data=df, x=\"log2 relative coexpression\", y=\"sample\", hue='segmentation', cut=0, width=.8,inner='quart')\n",
    "\n",
    "            plt.title(f\"{cohort=} {panel=} {method=} {target_count=}\")\n",
    "            # plt.axvline(0,c='k',linestyle='-',zorder=0,alpha=.3)\n",
    "            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Segmentation', frameon=False)\n",
    "            sns.despine(offset=10, trim=True)\n",
    "            plt.gca().xaxis.grid(True) \n",
    "            plt.savefig(cfg['figures_dir'] + f\"{cohort=} {panel=} {method=} {target_count=}.png\",dpi=300,bbox_inches='tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_genes_idx = sd1.var_names.isin(u_markers_panel)\n",
    "pos_nuc_markers = pos_nuc[:,marker_genes_idx]\n",
    "\n",
    "CC_cells = coexpression.coexpression_cells(pos_nuc)\n",
    "CC_cells_sum = coexpression.coexpression_cells_score(CC_cells, marker_genes_idx)\n",
    "plt.hist(CC_cells_sum,bins=50)\n",
    "plt.show()\n",
    "\n",
    "sd1.obs.loc[mask_nuc,'CC_cells_sum'] = CC_cells_sum\n",
    "\n",
    "sc.set_figure_params(figsize=(15,15))\n",
    "sc.pl.spatial(sd1[mask_nuc],spot_size=15,color='CC_cells_sum',cmap='viridis',vmax=50)\n",
    "sc.pl.spatial(sd1[mask_nuc],spot_size=15,color='total_counts',cmap='viridis',vmax=150)\n",
    "\n",
    "plt.scatter(sd1[mask_nuc].obs['total_counts'],sd1[mask_nuc].obs['CC_cells_sum'],alpha=.5)\n",
    "np.corrcoef(sd1[mask_nuc].obs['total_counts'],sd1[mask_nuc].obs['CC_cells_sum'])[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find coexpressed and spurious genes scRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = sc.read(cfg['scrnaseq_processed_flex_full_zurich_h5ad'])\n",
    "ad_panel = ad[:,[g for g in sd1.var_names if g in ad.var_names]]\n",
    "\n",
    "# compute CC\n",
    "CCnuc_found = CCnuc.loc[ad_panel.var_names,ad_panel.var_names]\n",
    "CCnuc_sc, Xnuc_downsampled_sc, pos_nuc_sc, pos_rate_nuc_sc, mask_nuc_sc = coexpression.coexpression(ad_panel,return_all=1)\n",
    "CCdiff_sc, spurious_gene_pairs_sc = coexpression.compare_segmentations(pos_rate_nuc_sc, pos_rate_nuc, CCnuc_sc, CCnuc_found, min_positivity_rate=0.01,cc_cutoff = 2)\n",
    "spurious_genes_idx_sc = np.unique(np.where(spurious_gene_pairs_sc))\n",
    "spurious_genes_sc = ad_panel.var_names[spurious_genes_idx_sc]\n",
    "spurious_genes_markers_sc = df_markers_panel.loc[df_markers_panel['canonical'].isin(spurious_genes_sc),'canonical'].unique()\n",
    "\n",
    "print(len(spurious_genes_markers_sc), 'marker genes /', len(spurious_genes_sc),'spurious panel genes found')\n",
    "\n",
    "# plot\n",
    "sns.set(font_scale=.8)\n",
    "# sns.clustermap(CCnuc.loc[spurious_genes,spurious_genes],yticklabels=True,xticklabels=True,figsize=(10,10))\n",
    "sns.clustermap(CCdiff_sc.loc[spurious_genes_markers_sc,spurious_genes_markers_sc],\n",
    "    yticklabels=True,xticklabels=True,figsize=(15,15),vmin=None,vmax=4,cmap='RdBu_r',center=1)\n",
    "plt.show()\n",
    "\n",
    "df_ct_markers_cc_nuc_sc, df_cc_melt_nuc_sc = coexpression.coexpression_by_cell_type(CCnuc_sc,spurious_genes_markers_sc,df_markers_panel)\n",
    "sns.heatmap(df_ct_markers_cc,cmap='Purples')\n",
    "plt.show()\n",
    "\n",
    "df_ct_markers_cc_diff_sc, df_cc_melt_diff_sc = coexpression.coexpression_by_cell_type(CCdiff_sc,spurious_genes_markers_sc,df_markers_panel)\n",
    "sns.heatmap(df_ct_markers_cc,cmap='RdBu_r',center=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "df_markers_panel_ct = df_markers_panel.set_index(\"canonical\")\n",
    "df_cc_melt = df_cc_melt_nuc_sc.copy()\n",
    "df_cc_melt['ct_genei'] = ''\n",
    "df_cc_melt['ct_genej'] = ''\n",
    "for i, row in df_cc_melt.iterrows():\n",
    "    ct_genei = df_markers_panel_ct.loc[row[\"genei\"], \"index\"]\n",
    "    ct_genej = df_markers_panel_ct.loc[row[\"genej\"], \"index\"]\n",
    "\n",
    "    if isinstance(ct_genei, str):\n",
    "        ct_genei = [ct_genei]\n",
    "    if isinstance(ct_genej, str):\n",
    "        ct_genej = [ct_genej]\n",
    "    df_cc_melt.loc[df_cc_melt[\"genei\"] == row[\"genei\"],'ct_genei'] = ' & '.join(ct_genei) \n",
    "    df_cc_melt.loc[df_cc_melt[\"genej\"] == row[\"genej\"],'ct_genej'] = ' & '.join(ct_genej)\n",
    "\n",
    "display(df_cc_melt[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coexpression based on computational marker genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate genes coexpression from scRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doublets\n",
    "# result = pyreadr.read_r('/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/dbuszta/meso/processed_data/new_data/doublets_by_sample.rds')[None]\n",
    "\n",
    "# load data\n",
    "ad_sc = sc.read_h5ad(cfg['scrnaseq_processed_flex_full_zurich_h5ad'])\n",
    "# ad_sc.obs['doublet_status'] = result.iloc[:,0]\n",
    "# ad_sc = ad_sc[ad_sc.obs['doublet_status']=='singlet']\n",
    "# ad_sc.obs = ad_sc.obs.join(pd.read_csv(cfg['results_dir']+'annotation/cell_type_annotation.csv',index_col=0))\n",
    "\n",
    "# filter to xenium genes\n",
    "xenium_panel_genes = pd.read_csv(cfg['data_dir']+'xenium_chuvio_panel_genes.csv',index_col=0)['0']\n",
    "ad_sc_panel = ad_sc[:,ad_sc.var_names.isin(xenium_panel_genes)]\n",
    "\n",
    "CT_KEY = 'cell_type_harmonized_level2_consensus'\n",
    "u_cell_types = ad_sc_panel.obs[CT_KEY].unique()\n",
    "\n",
    "# get df with max fraction for each cell type\n",
    "(df_fraction,df_difference, \n",
    "df_max_difference) = coexpression.find_markers(ad_sc_panel, CT_KEY, threshold_fraction = 1., threshold_diff = 0.)\n",
    "df_difference_sorted = df_difference[df_difference.max().sort_values().index]\n",
    "\n",
    "# get df with thresholded max fraction for each cell type\n",
    "(_,df_difference_thresholded, \n",
    " df_max_difference_thresholded) = coexpression.find_markers(ad_sc_panel, CT_KEY, threshold_fraction = 0.01, threshold_diff = 0.25)\n",
    "df_difference_thresholded_sorted = df_difference_thresholded[df_difference_thresholded.max().sort_values().index]\n",
    "\n",
    "for df_difference_sorted_, df_max_difference_ in zip([df_difference_sorted,df_difference_thresholded_sorted],\n",
    "                                                     [df_max_difference,df_max_difference_thresholded]):\n",
    "\n",
    "    ### Boxplots\n",
    "    plt.figure(figsize=(8,30))\n",
    "\n",
    "    flierprops = dict(marker='o', markersize=1, linestyle='none', markerfacecolor='black')\n",
    "    ax = sns.boxplot(data=df_difference_sorted_, orient='h', flierprops=flierprops)\n",
    "\n",
    "    # Loop over columns to find and highlight max fliers\n",
    "    for i, col in enumerate(df_difference_sorted_.columns):\n",
    "        data = df_difference_sorted_[col]\n",
    "        max_val = data.max()  # Find maximum value in the column\n",
    "        ax.scatter(max_val,i, color='red', edgecolor='black', zorder=5, s=20)\n",
    "\n",
    "    plt.title(\"Difference in % of cells expressing\")\n",
    "    plt.show()\n",
    "\n",
    "    ### Heatmap\n",
    "    sns.heatmap(df_max_difference_)\n",
    "\n",
    "# Define markers\n",
    "df_markers_scores = pd.DataFrame(index=ad_sc_panel.var_names,columns=u_cell_types)\n",
    "\n",
    "for cti in u_cell_types:\n",
    "    cti_columns = df_difference_thresholded.columns.str.split(\"_\").str[0] == cti\n",
    "\n",
    "    df_markers_scores[cti] = df_difference_thresholded.loc[:,cti_columns].mean(1)\n",
    "\n",
    "ct_markers = {}\n",
    "for ct in u_cell_types:\n",
    "    ct_scores = df_markers_scores[ct].sort_values(ascending=False).dropna()\n",
    "    markers_ = list(ct_scores[(ct_scores>0.)][:10].index)\n",
    "\n",
    "    if len(markers_) > 0:\n",
    "        ct_markers[ct] = markers_\n",
    "\n",
    "df_markers_computational = pd.DataFrame([\n",
    "    {\"index\": k, \"canonical\": v}\n",
    "    for k, values in ct_markers.items()\n",
    "    for v in values\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coexpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rctd_weights_key = 'weights_FLEX_original'\n",
    "\n",
    "normalized_entropy_computational = {}\n",
    "CC_computational = {}\n",
    "CC_cells_sum_normalized_computational = {}\n",
    "\n",
    "for segmentation in SEGMENTATIONS:\n",
    "\n",
    "    normalized_entropy_computational[segmentation] = {}\n",
    "    CC_computational[segmentation] = {}\n",
    "    CC_cells_sum_normalized_computational[segmentation] = {}\n",
    "        \n",
    "    for k,ad_ in ads[segmentation].items():\n",
    "        print(k)\n",
    "\n",
    "        ad = ad_[ad_.uns[rctd_weights_key].index]\n",
    "\n",
    "        ### Entropy\n",
    "        W = ad.uns[rctd_weights_key].copy()\n",
    "        W[W<0] = 0.\n",
    "        W = sklearn.preprocessing.normalize(W,axis=0,norm='l1')\n",
    "        normalized_entropy_computational[segmentation][k] = scipy.stats.entropy(W,axis=1)/np.log(W.shape[1])\n",
    "\n",
    "        ### Coexpression\n",
    "        # compute CC\n",
    "        CC, X_downsampled, pos, pos_rate, mask = coexpression.coexpression(ad,target_count=30,method='jaccard')\n",
    "        CC_computational[segmentation][k] = CC\n",
    "\n",
    "        # load markers\n",
    "        # df_markers_computational_panel = df_markers_computational[df_markers_computational['canonical'].isin(ad.var_names)]\n",
    "        # u_markers_panel = df_markers_computational_panel['canonical'].unique()\n",
    "        # marker_genes_idx = ad.var_names.isin(u_markers_panel)\n",
    "        # print(len(u_markers_panel), 'marker genes /', ad.shape[1],'panel genes found')\n",
    "\n",
    "        # df_ct_markers_cc = coexpression.coexpression_by_cell_type(CC,u_markers_panel,df_markers_computational_panel)\n",
    "#         CC_cells = coexpression.coexpression_cells(pos)\n",
    "#         CC_cells_sum = coexpression.coexpression_cells_score(CC_cells, marker_genes_idx)\n",
    "#         CC_cells_sum_normalized_computational[k] = np.array(CC_cells_sum)/(marker_genes_idx.sum()**2)\n",
    "#         break\n",
    "\n",
    "# palette = {'singlet':'green','doublet_certain':'blue','doublet_uncertain':'orange','reject':'red'}\n",
    "# for segmentation in SEGMENTATIONS[:1]:\n",
    "#     for k,ad_ in ads[segmentation].items():\n",
    "#         ad = ad_[ad_.uns[rctd_weights_key].index]\n",
    "\n",
    "#         g = sns.jointplot(x=normalized_entropy_computational[k], y=CC_cells_sum_normalized_computational[k], palette=palette,hue=ad.obs['spot_class_FLEX_original'],kind=\"scatter\", height=5,s=8,alpha=.3)\n",
    "#         g.set_axis_labels(f'{k} - normalized RCTD weights normalized entropy', f'{k} - normalized coexpression score')\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rctd_weights_key = 'weights_FLEX_original'\n",
    "\n",
    "# Define a function for processing one (segmentation, k) pair\n",
    "def process_task(segmentation, k, ad_, rctd_weights_key):\n",
    "    print(f\"Processing {segmentation}, {k}\")\n",
    "\n",
    "    # Filter data\n",
    "    ad = ad_[ad_.uns[rctd_weights_key].index]\n",
    "\n",
    "    # Entropy computation\n",
    "    W = ad.uns[rctd_weights_key].copy()\n",
    "    W[W < 0] = 0.0\n",
    "    W = sklearn.preprocessing.normalize(W, axis=0, norm='l1')\n",
    "    normalized_entropy = scipy.stats.entropy(W, axis=1) / np.log(W.shape[1])\n",
    "\n",
    "    # Coexpression computation\n",
    "    CC, X_downsampled, pos, pos_rate, mask = coexpression.coexpression(ad, target_count=30, method='jaccard')\n",
    "\n",
    "    return segmentation, k, normalized_entropy, CC\n",
    "\n",
    "# Prepare the tasks\n",
    "tasks = []\n",
    "for segmentation in SEGMENTATIONS:\n",
    "    for k, ad_ in ads[segmentation].items():\n",
    "        tasks.append((segmentation, k, ad_, rctd_weights_key))\n",
    "\n",
    "# Run tasks in parallel\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(process_task)(segmentation, k, ad_, rctd_weights_key) for segmentation, k, ad_, rctd_weights_key in tasks\n",
    ")\n",
    "\n",
    "# Collect results\n",
    "normalized_entropy_computational = {seg: {} for seg in SEGMENTATIONS}\n",
    "CC_computational = {seg: {} for seg in SEGMENTATIONS}\n",
    "\n",
    "for segmentation, k, entropy, CC in results:\n",
    "    normalized_entropy_computational[segmentation][k] = entropy\n",
    "    CC_computational[segmentation][k] = CC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '16MD_run14'\n",
    "coexpr_scores = pd.DataFrame()\n",
    "normalized_entropy_computational_scores = pd.DataFrame()\n",
    "for segmentation,CC_segmentations in CC_computational.items():\n",
    "    for sample, CC_sample in CC_segmentations.items():\n",
    "        coexpr_scores.loc[segmentation,sample] = CC_sample.values[np.triu_indices_from(CC_sample,1)].sum()\n",
    "        normalized_entropy_computational_scores.loc[segmentation,sample] = normalized_entropy_computational[segmentation][sample].sum()\n",
    "\n",
    "sns.clustermap(coexpr_scores.fillna(coexpr_scores.mean()),figsize=(10,5),cmap='Reds')\n",
    "\n",
    "coexpr_scores_norm = coexpr_scores.fillna(coexpr_scores.mean())\n",
    "sns.clustermap(coexpr_scores_norm/coexpr_scores_norm.max(),figsize=(10,5),cmap='Reds')\n",
    "\n",
    "sns.clustermap(normalized_entropy_computational_scores,figsize=(10,5),cmap='Reds')\n",
    "sns.clustermap(normalized_entropy_computational_scores/normalized_entropy_computational_scores.max(),figsize=(10,5),cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_transcripts_per_cell = pd.DataFrame()\n",
    "for segmentation, ads_seg in ads.items():\n",
    "    for sample, ad in ads_seg.items():\n",
    "        median_transcripts_per_cell.loc[segmentation,sample] = ad.uns['metrics_summary'].loc[0,'median_transcripts_per_cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = normalized_entropy_computational_scores\n",
    "df2 = coexpr_scores\n",
    "\n",
    "plot_data = []\n",
    "for idx in df1.index:\n",
    "    for col in df1.columns:\n",
    "        plot_data.append({\n",
    "            'segmentation': idx,\n",
    "            'samples': col,\n",
    "            'normalized_entropy_computational_scores': df1.loc[idx, col],\n",
    "            'coexpr_scores': df2.loc[idx, col]\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "sns.boxplot(plot_df,x='segmentation',y='coexpr_scores');plt.show()\n",
    "sns.boxplot(plot_df,x='segmentation',y='normalized_entropy_computational_scores')\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x='normalized_entropy_computational_scores',  # x-axis: values from df1\n",
    "    y='coexpr_scores',  # y-axis: values from df2\n",
    "    hue='segmentation',  # color based on segmentation\n",
    "    style='samples',  # marker type based on samples\n",
    "    s=50  # marker size\n",
    ")\n",
    "\n",
    "# plt.title('Scatter Plot: df1 vs df2')\n",
    "# plt.xlabel('Values from df1')\n",
    "# plt.ylabel('Values from df2')\n",
    "plt.legend(title='Legend', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find universal markers\n",
    "ad_sc_panel.var['n_cells'] = (ad_sc_panel.X>0).sum(0).A1\n",
    "ad_sc_panel.var['n_cells'].hist(bins=100)\n",
    "CC, X_downsampled, pos_, pos_rate_, mask_ = coexpression.coexpression(ad_sc_panel,target_count=None,min_cond_coex=0.)\n",
    "\n",
    "threshold = 0.01\n",
    "pos_rate_threshold = 0.01\n",
    "\n",
    "valid_genes = np.where(pos_rate > pos_rate_threshold)[0]\n",
    "\n",
    "gene_pairs_idx = np.vstack((CC.values<threshold).nonzero()).T\n",
    "gene_pairs_idx_u = pd.DataFrame(gene_pairs_idx)\n",
    "gene_pairs_idx_u['Key'] = gene_pairs_idx_u.apply(lambda row: tuple(sorted(row)), axis=1)\n",
    "gene_pairs_idx_u = gene_pairs_idx_u.drop_duplicates(subset='Key').drop(columns='Key')\n",
    "gene_pairs_idx_u = gene_pairs_idx_u[gene_pairs_idx_u.isin(valid_genes).sum(1)==2]\n",
    "gene_pairs_idx_u = gene_pairs_idx_u.values\n",
    "print(len(gene_pairs_idx_u))\n",
    "\n",
    "# Score xenium samples per gene\n",
    "# CC_computational_scores = {}\n",
    "# for segmentation in SEGMENTATIONS:\n",
    "#     for k,ad_ in ads[segmentation].items():\n",
    "#         CC_computational_scores[segmentation][k] = CC_computational[segmentation][k].values[gene_pairs_idx_u[:,0],gene_pairs_idx_u[:,1]].sum(1)\n",
    "\n",
    "# Score xenium samples per cell\n",
    "# CC_cells_score = coexpression.coexpression_cells_score_gene_pairs(CC_cells, gene_pairs_idx_u)\n",
    "\n",
    "# ad.obs['CC_cells_score'] = CC_cells_score\n",
    "# ad.obs['CC_cells_score'].hist(bins=100)\n",
    "\n",
    "# sc.set_figure_params(figsize=(10,10))\n",
    "# sc.pl.spatial(ad,color='CC_cells_score',spot_size=10,)\n",
    "# (ad.obs['CC_cells_score']>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
